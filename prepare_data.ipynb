{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870761a3-e1e0-44c9-86dd-1817ca621da4",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58767cb-2009-4d9b-b2ba-8825906939f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956beb28-90aa-4baf-9c7c-321a9a8ecb6a",
   "metadata": {},
   "source": [
    "Create a conda environment to do the CLIP embedding.\n",
    "This won't be used after the vectors are generated.\n",
    "\n",
    "To install the conda environment, run:\n",
    "```shell\n",
    "source /opt/conda/bin/activate\n",
    "conda create --yes -n clip\n",
    "conda activate clip\n",
    "conda install --yes -c pytorch torchvision cudatoolkit ipykernel pandas pyarrow\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0586-ed73-4fa0-aa18-4f1dc5f15a17",
   "metadata": {},
   "source": [
    "Install instructions from https://github.com/openai/CLIP/tree/main#usage\n",
    "\n",
    "To add a conda environment as a kernel for jupyter, run:\n",
    "```shell\n",
    "conda activate clip\n",
    "conda install ipykernel\n",
    "python -m ipykernel install --user --name clip --display-name clip\n",
    "```\n",
    "Then in the top right corner of the notebook, switch the kernel to `clip`.\n",
    "\n",
    "To list the installed kernels (requires `jupyter` be installed), run:\n",
    "```shell\n",
    "jupyter kernelspec list\n",
    "```\n",
    "To remove an installed kernel, run:\n",
    "```shell\n",
    "jupyter kernelspec uninstall clip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05210d-5ee7-4188-bb79-e7c6f2b1132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d34c0d-9f79-4696-b755-8c7fb2c3d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) â†’ ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd519b-2a03-4149-8a41-228dc7b966bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68d7cb-8df3-409b-853d-d03b2e2cb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85b7a2-f3c5-4a75-99d9-af22c167b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7dad7c-bfe1-4672-b208-8c55fee02995",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\",\n",
    "      f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e6601-c3d0-4466-aa2e-45d3b36f4652",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea821b-24d1-48e7-b5d3-5bac010d87ff",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c77da0-0338-43e6-9e8e-2f54ad561117",
   "metadata": {},
   "source": [
    "The [MNIST Dataset](http://yann.lecun.com/exdb/mnist/) contains 70,000 images of handwritten digits divided across 10 classes. Each class contains roughly 7,000 images. It is further divided by writer into 60,000 train and 10,000 test examples.\n",
    "\n",
    "```\n",
    "@misc{mnist,\n",
    "  title={The MNIST database of handwritten digits},\n",
    "  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
    "  howpublished={\\url{http://yann.lecun.com/exdb/mnist/}},\n",
    "  year={1998},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d085b-f98e-4581-88cd-9265be13d315",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7000f14-f516-4aae-a95e-b7ea8025531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = (\n",
    "    \"train-images-idx3-ubyte.gz\",\n",
    "    \"train-labels-idx1-ubyte.gz\",\n",
    "    \"t10k-images-idx3-ubyte.gz\",\n",
    "    \"t10k-labels-idx1-ubyte.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33dbb6-af02-4b3a-974b-23c5adbb491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://yann.lecun.com/exdb/mnist/\n",
    "# has blacklisted the default urllib user agent.\n",
    "# https://github.com/pytorch/vision/issues/3497#issuecomment-789996883\n",
    "# Use the PyTorch mirror instead.\n",
    "for filename in filenames:\n",
    "    url = f\"https://storage.googleapis.com/cvdf-datasets/mnist/{filename}\"\n",
    "    _ = urllib.request.urlretrieve(url, str(data_path / \"mnist\" / filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804eb49-a2be-4e13-8386-23b0ac2547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "images = dict()\n",
    "labels = dict()\n",
    "n_examples = {\"train\": 60_000, \"test\": 10_000}\n",
    "for filename in filenames:\n",
    "    if filename.startswith(\"train\"):\n",
    "        split = \"train\"\n",
    "    elif filename.startswith(\"t10k\"):\n",
    "        # Are test examples.\n",
    "        split = \"test\"\n",
    "    with gzip.open(data_path / \"mnist\" / filename, \"rb\") as file:\n",
    "        if \"images\" in filename:\n",
    "            # Skip header.\n",
    "            file.read(16)\n",
    "            buffer = file.read(image_size * image_size * n_examples[split])\n",
    "            images[split] = np.frombuffer(\n",
    "                buffer,\n",
    "                dtype=np.uint8,\n",
    "            ).reshape(n_examples[split], image_size, image_size)\n",
    "        elif \"labels\" in filename:\n",
    "            # Skip header.\n",
    "            file.read(8)\n",
    "            buffer = file.read(n_examples[split])\n",
    "            labels[split] = np.frombuffer(buffer, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63090f22-ee34-4cd7-a1d0-2deb4f83cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    \"class\": labels[\"test\"],\n",
    "    \"split\": \"test\",\n",
    "    \"image\": [image for image in images[\"test\"]],\n",
    "})\n",
    "df_train = pd.DataFrame({\n",
    "    \"class\": labels[\"train\"],\n",
    "    \"split\": \"train\",\n",
    "    \"image\": [image for image in images[\"train\"]],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2159b8f-524c-4d5c-8f18-bdf2dfbfcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0de4c-4b8e-4834-af1c-1c6f8af4c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"image\"] = df[\"image\"].apply(lambda x: np.array(x, dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fdb71-b435-4799-a987-82267a4df5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"class\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e94a2-ac98-4b8d-ba3b-29e248bd0991",
   "metadata": {},
   "source": [
    "### Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30dc24-c630-4a0d-b5e5-b6edb258a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vectors = []\n",
    "n = 64\n",
    "total = len(df[\"image\"]) // n + (len(df[\"image\"]) % n > 0)\n",
    "for batch in tqdm(batched(df[\"image\"], n), total=total):\n",
    "    image_input = [preprocess(Image.fromarray(x, mode=\"L\")) for x in batch]\n",
    "    image_features = model.encode_image(\n",
    "        torch.stack(image_input).to(device)).detach().numpy()\n",
    "    clip_vectors.append(image_features)\n",
    "clip_vectors = np.vstack(clip_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935edf6-5dba-496d-8d6d-2ef2f6e3cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clip\"] = clip_vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1156736-1285-484e-8a40-ee6a1dc58baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clip_norm\"] = (\n",
    "    clip_vectors /\n",
    "    np.linalg.norm(clip_vectors, axis=1, keepdims=True)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510edc2-5718-493e-8ac8-0ee3b9097508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"image\"] = df[\"image\"].apply(lambda x: x.reshape((28, 28)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa10fc3-f0f0-4f2f-8468-55a1da098d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(data_path / \"mnist\" / \"mnist.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e581f69b-cabc-463f-9601-322a6c17e75d",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038df009-000c-4cf2-a417-7ff8883d9cfb",
   "metadata": {},
   "source": [
    "PyTorch expects tensors to be in channels first format, unlike TensorFlow which uses channels last. https://pytorch.org/blog/tensor-memory-format-matters/\n",
    "\n",
    "`preprocess` takes care of grayscale to RGB conversion, channel first formatting, and ensuring the image size is larger than the kernel size of (32 x 32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3abda-a161-47ef-a0e3-d3ef9983352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_to_rgb(image: np.ndarray):\n",
    "    \"\"\"Converts a grayscale image to RGB.\n",
    "\n",
    "    Repeats the image over all 3 channels like\n",
    "    tensorflow.image.grayscale_to_rgb .\n",
    "    The input images' last dimension must be size 1.\n",
    "    \"\"\"\n",
    "    return np.repeat(image, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2088d81-75fa-4ab5-9fab-888eb05bb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(df.loc[0, \"image\"], cmap=\"gray\")\n",
    "plt.title(f'Digit: {df.loc[0, \"class\"]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373a1a7-03d3-4e58-b1e8-43e91383b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(preprocess(Image.fromarray(df.loc[0, \"image\"], mode=\"L\")).numpy()[1], cmap=\"gray\")\n",
    "plt.title(f'Digit: {df.loc[0, \"class\"]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14563d2c-7dd4-4ffc-ba5f-517fe86c0c21",
   "metadata": {},
   "source": [
    "## Pets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb5ffd-340c-42f2-8521-872054b2f440",
   "metadata": {},
   "source": [
    "The [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) contains 7,390 images divided across 37 classes of cat and dog breeds. Each class contains roughly 200 images.\n",
    "\n",
    "```\n",
    "@misc{pets,\n",
    "  title={The Oxford-IIIT PET Dataset},\n",
    "  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},\n",
    "  howpublished={\\url{https://www.robots.ox.ac.uk/~vgg/data/pets/}},\n",
    "  year={2012},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56725e54-488b-442a-8e39-be6db6cc96e2",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceaf63b-cd5c-4fef-91b9-bb5005211b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (data_path / \"pets/annotations/\").exists():\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\",\n",
    "        str(data_path / \"pets\" / \"annotations.tar.gz\"))\n",
    "\n",
    "    tar = tarfile.open(str(data_path / \"pets\" / \"annotations.tar.gz\"), \"r:gz\")\n",
    "    tar.extractall(path=str(data_path / \"pets\"))\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc1a7a-5fe8-4d8b-85a8-21a0cec65be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (data_path / \"pets/images/\").exists():\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\",\n",
    "        str(data_path / \"pets\" / \"images.tar.gz\"))\n",
    "\n",
    "    tar = tarfile.open(str(data_path / \"pets\" / \"images.tar.gz\", \"r:gz\")\n",
    "    tar.extractall(path=str(data_path / \"pets\"))\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b6514-a323-4e74-990e-50950a3257b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.read_csv(\n",
    "    data_path / \"pets\" / \"annotations\" / \"list.txt\",\n",
    "    sep=\" \",\n",
    "    comment=\"#\",\n",
    "    names=[\"image\", \"class_id\", \"species_id\", \"breed_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d7fd8-bc24-46a9-91ae-5025dfc469a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations[\"class\"] = (\n",
    "    df_annotations[\"image\"].str.split(\"_\").str[:-1].str.join(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7be5a-994d-423b-912b-1897f4d56a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = df_annotations[[\"class\", \"class_id\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f62067-c5ba-4e6a-993f-6cb7d98c57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = {\n",
    "    row[\"class_id\"]: row[\"class\"] for _, row in class_labels.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a173bff-b55f-4047-8820-7585c8ae9a76",
   "metadata": {},
   "source": [
    "`class_id` is a global unique class id from 1 to 37, `species_id` is either 1 for cat or 2 for dog, `breed_id` is a class id that is only unique given the species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c798c85-705f-4c7d-ac1e-2db921ee8e41",
   "metadata": {},
   "source": [
    "### Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a408c-21ff-41ad-9e56-e191fd58b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list((data_path / \"pets\" / \"images\").glob(\"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40a5ab-4132-479d-972b-02e2ac872bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vectors = []\n",
    "n = 64\n",
    "total = len(image_paths) // n + (len(image_paths) % n > 0)\n",
    "for batch in tqdm(batched(image_paths, n), total=total):\n",
    "    image_input = [preprocess(Image.open(x)) for x in batch]\n",
    "    image_features = model.encode_image(\n",
    "        torch.stack(image_input).to(device)).detach().numpy()\n",
    "    clip_vectors.append(image_features)\n",
    "clip_vectors = np.vstack(clip_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f979c57-5b3b-4734-90df-6587ba93a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"path\": [str(x) for x in image_paths],\n",
    "    \"image\": [x.stem for x in image_paths]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0d42d-3e2e-4198-a987-755142496beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clip\"] = clip_vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f7f6e-2525-4c56-92ff-460440af5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clip_norm\"] = (\n",
    "    clip_vectors /\n",
    "    np.linalg.norm(clip_vectors, axis=1, keepdims=True)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8ab38-932b-4551-9e46-9a0898c3abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"image\"].str.split(\"_\").str[:-1].str.join(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6fe35-9b93-4d7e-9a1e-30631892b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"species\"] = df[\"class\"].str[0].str.isupper().map({\n",
    "    True: \"cat\",\n",
    "    False: \"dog\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a4452-5b77-447f-82c0-e60417447a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"image_n\"] = df[\"image\"].str.split(\"_\").str[-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca55ce2-3526-4cce-90ec-85d464e89046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"class\", \"image_n\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605eac0-5fcb-40e2-a1b3-97087f86ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(data_path / \"pets\" / \"pets_clip.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fb408-c347-41b9-90a8-8af5086fe3bf",
   "metadata": {},
   "source": [
    "### Incomplete Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c883f9b-dbea-4324-afcf-fa4b3ed7c5ba",
   "metadata": {},
   "source": [
    "Some images were not listed in the given annotations. The given annotations were discarded and complete ones built instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ff7ad-f4f3-4bd4-b234-2c997a7c8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of annotations: {df_annotations.shape[0]}\")\n",
    "print(f\"Number of images: {clip_vectors.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec667e24-6b6b-4485-b614-6292baf37d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = (data_path / \"pets/annotations/test.txt\").read_text().count(\"\\n\")\n",
    "n_train = (data_path / \"pets/annotations/trainval.txt\").read_text().count(\"\\n\")\n",
    "print(n_test, n_train, n_test + n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61431ba1-fea5-4f25-a37a-b2a3303b531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df[\"image\"].isin(df_annotations[\"image\"]), \"image\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c281a98-f06b-478c-91ad-6d73c4210e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.loc[~df_annotations[\"image\"].isin(df[\"image\"]), \"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeece4bb-5b69-4cb1-b496-54bd9c9a7c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations.groupby(\"class\").size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96254e9a-409b-44ea-82dd-225c45ed5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"class\").size().sort_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
