{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870761a3-e1e0-44c9-86dd-1817ca621da4",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835ada3-0a28-428b-b703-7d27807291f6",
   "metadata": {},
   "source": [
    "The [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) contains 37 categories of cat and dog breeds of roughly 200 images per class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58767cb-2009-4d9b-b2ba-8825906939f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956beb28-90aa-4baf-9c7c-321a9a8ecb6a",
   "metadata": {},
   "source": [
    "Create a conda environment to do the CLIP embedding.\n",
    "This won't be used after the vectors are generated.\n",
    "\n",
    "To install the conda environment, run:\n",
    "```shell\n",
    "source /opt/conda/bin/activate\n",
    "conda create -n clip\n",
    "conda activate clip\n",
    "conda install --yes -c pytorch torchvision cudatoolkit ipykernel pandas pyarrow\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0586-ed73-4fa0-aa18-4f1dc5f15a17",
   "metadata": {},
   "source": [
    "Install instructions from https://github.com/openai/CLIP/tree/main#usage\n",
    "\n",
    "To add a conda environment as a kernel for jupyter, run:\n",
    "```shell\n",
    "conda activate clip\n",
    "conda install ipykernel\n",
    "python -m ipykernel install --user --name clip --display-name clip\n",
    "```\n",
    "Then in the top right corner of the notebook, switch the kernel to `clip`.\n",
    "\n",
    "To list the installed kernels (requires `jupyter` be installed), run:\n",
    "```shell\n",
    "jupyter kernelspec list\n",
    "```\n",
    "To remove an installed kernel, run:\n",
    "```shell\n",
    "jupyter kernelspec uninstall clip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f05210d-5ee7-4188-bb79-e7c6f2b1132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56725e54-488b-442a-8e39-be6db6cc96e2",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ceaf63b-cd5c-4fef-91b9-bb5005211b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data/annotations/\").exists():\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\",\n",
    "        \"data/annotations.tar.gz\")\n",
    "\n",
    "    tar = tarfile.open(\"data/annotations.tar.gz\", \"r:gz\")\n",
    "    tar.extractall(path=\"data/\")\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacc1a7a-5fe8-4d8b-85a8-21a0cec65be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data/images/\").exists():\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\",\n",
    "        \"data/images.tar.gz\")\n",
    "\n",
    "    tar = tarfile.open(\"data/images.tar.gz\", \"r:gz\")\n",
    "    tar.extractall(path=\"data/\")\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051b6514-a323-4e74-990e-50950a3257b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = pd.read_csv(\n",
    "    \"data/annotations/list.txt\",\n",
    "    sep=\" \",\n",
    "    comment=\"#\",\n",
    "    names=[\"image\", \"class_id\", \"species_id\", \"breed_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99d7fd8-bc24-46a9-91ae-5025dfc469a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations[\"class\"] = df_annotations[\"image\"].str.split(\n",
    "    \"_\").str[:-1].str.join(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b7be5a-994d-423b-912b-1897f4d56a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = df_annotations[[\"class\", \"class_id\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f62067-c5ba-4e6a-993f-6cb7d98c57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = {\n",
    "    row[\"class_id\"]: row[\"class\"] for _, row in class_labels.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a173bff-b55f-4047-8820-7585c8ae9a76",
   "metadata": {},
   "source": [
    "`class_id` is a global unique class id from 1 to 37, `species_id` is either 1 for cat or 2 for dog, `breed_id` is a class id that is only unique given the species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c798c85-705f-4c7d-ac1e-2db921ee8e41",
   "metadata": {},
   "source": [
    "## CLIP Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7fd519b-2a03-4149-8a41-228dc7b966bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f85b7a2-f3c5-4a75-99d9-af22c167b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d4a408c-21ff-41ad-9e56-e191fd58b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = list(Path(\"data/images\").glob(\"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d34c0d-9f79-4696-b755-8c7fb2c3d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c40a5ab-4132-479d-972b-02e2ac872bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [01:49<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "clip_vectors = []\n",
    "n = 64\n",
    "total = len(image_paths) // n + (len(image_paths) % n > 0)\n",
    "for batch in tqdm(batched(image_paths, n), total=total):\n",
    "    image_input = [preprocess(Image.open(x)) for x in batch]\n",
    "    image_features = model.encode_image(\n",
    "        torch.stack(image_input).to(device)).detach().numpy()\n",
    "    clip_vectors.append(image_features)\n",
    "clip_vectors = np.vstack(clip_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f979c57-5b3b-4734-90df-6587ba93a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"path\": [str(x) for x in image_paths],\n",
    "    \"image\": [x.stem for x in image_paths]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e0d42d-3e2e-4198-a987-755142496beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clip\"] = clip_vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5f7f6e-2525-4c56-92ff-460440af5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clip_norm\"] = (\n",
    "    clip_vectors /\n",
    "    np.linalg.norm(clip_vectors, axis=1, keepdims=True)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebe8ab38-932b-4551-9e46-9a0898c3abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"image\"].str.split(\"_\").str[:-1].str.join(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d6fe35-9b93-4d7e-9a1e-30631892b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"species\"] = df[\"class\"].str[0].str.isupper().map({\n",
    "    True: \"cat\",\n",
    "    False: \"dog\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e25a4452-5b77-447f-82c0-e60417447a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"image_n\"] = df[\"image\"].str.split(\"_\").str[-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ca55ce2-3526-4cce-90ec-85d464e89046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values([\"class\", \"image_n\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6605eac0-5fcb-40e2-a1b3-97087f86ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/clip.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fb408-c347-41b9-90a8-8af5086fe3bf",
   "metadata": {},
   "source": [
    "## Incomplete Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c883f9b-dbea-4324-afcf-fa4b3ed7c5ba",
   "metadata": {},
   "source": [
    "Some images were not listed in the given annotations. The given annotations were discarded and complete ones built instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c68ff7ad-f4f3-4bd4-b234-2c997a7c8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 7349\n",
      "Number of images: 7390\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of annotations: {df_annotations.shape[0]}\")\n",
    "print(f\"Number of images: {clip_vectors.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb9582ad-2638-4b4d-a37d-6231c889cf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3669 data/annotations/test.txt\n",
      "  3680 data/annotations/trainval.txt\n",
      "  7349 total\n"
     ]
    }
   ],
   "source": [
    "! wc --lines \"data/annotations/test.txt\" \"data/annotations/trainval.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61431ba1-fea5-4f25-a37a-b2a3303b531e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[~df[\"image\"].isin(df_annotations[\"image\"]), \"image\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c281a98-f06b-478c-91ad-6d73c4210e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: image, dtype: object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations.loc[~df_annotations[\"image\"].isin(df[\"image\"]), \"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeece4bb-5b69-4cb1-b496-54bd9c9a7c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Bombay                        184\n",
       "staffordshire_bull_terrier    189\n",
       "Egyptian_Mau                  190\n",
       "newfoundland                  196\n",
       "english_cocker_spaniel        196\n",
       "Abyssinian                    198\n",
       "boxer                         199\n",
       "keeshond                      199\n",
       "scottish_terrier              199\n",
       "Siamese                       199\n",
       "Persian                       200\n",
       "Ragdoll                       200\n",
       "Birman                        200\n",
       "British_Shorthair             200\n",
       "Russian_Blue                  200\n",
       "Sphynx                        200\n",
       "beagle                        200\n",
       "basset_hound                  200\n",
       "american_pit_bull_terrier     200\n",
       "american_bulldog              200\n",
       "german_shorthaired            200\n",
       "great_pyrenees                200\n",
       "chihuahua                     200\n",
       "english_setter                200\n",
       "japanese_chin                 200\n",
       "havanese                      200\n",
       "leonberger                    200\n",
       "miniature_pinscher            200\n",
       "pomeranian                    200\n",
       "pug                           200\n",
       "Maine_Coon                    200\n",
       "Bengal                        200\n",
       "samoyed                       200\n",
       "saint_bernard                 200\n",
       "shiba_inu                     200\n",
       "wheaten_terrier               200\n",
       "yorkshire_terrier             200\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations.groupby(\"class\").size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96254e9a-409b-44ea-82dd-225c45ed5200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "staffordshire_bull_terrier    191\n",
       "scottish_terrier              199\n",
       "Abyssinian                    200\n",
       "Bengal                        200\n",
       "British_Shorthair             200\n",
       "Egyptian_Mau                  200\n",
       "Birman                        200\n",
       "Bombay                        200\n",
       "Ragdoll                       200\n",
       "Russian_Blue                  200\n",
       "Siamese                       200\n",
       "Sphynx                        200\n",
       "american_bulldog              200\n",
       "american_pit_bull_terrier     200\n",
       "Maine_Coon                    200\n",
       "Persian                       200\n",
       "boxer                         200\n",
       "chihuahua                     200\n",
       "english_cocker_spaniel        200\n",
       "english_setter                200\n",
       "german_shorthaired            200\n",
       "great_pyrenees                200\n",
       "havanese                      200\n",
       "japanese_chin                 200\n",
       "keeshond                      200\n",
       "leonberger                    200\n",
       "miniature_pinscher            200\n",
       "newfoundland                  200\n",
       "pomeranian                    200\n",
       "pug                           200\n",
       "basset_hound                  200\n",
       "beagle                        200\n",
       "samoyed                       200\n",
       "saint_bernard                 200\n",
       "shiba_inu                     200\n",
       "wheaten_terrier               200\n",
       "yorkshire_terrier             200\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"class\").size().sort_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
